{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这是关于评价模型的记录。\n",
    "模型一共有以下几类：\n",
    "- 主成分分析\n",
    "- 因子分析\n",
    "- DEA\n",
    "- 熵值法\n",
    "- 层次分析法\n",
    "- 物元法\n",
    "- 模糊评判法"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 熵权法：客观赋权"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "核心思想：通过数据的熵来选取其权重 -> 局部差异大的数据更为重要"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本步骤：\n",
    "1. 选取指标：剔除次要指标\n",
    "   1. 方法：最小均方差法/极大极小离差法\n",
    "   2. 结果：m个主要指标${x_1, x_2, \\cdots, x_n}$, n个样本\n",
    "2. 极差变换法\n",
    "   1. 极大型指标：$$a_{i j}^*=\\frac{a_{i j}-\\min _{1 \\leq i \\leq n} a_{i j}}{\\max _{1 \\leq i \\leq n} a_{i j}-\\min _{1 \\leq i \\leq n} a_{i j}}(1 \\leq i \\leq n, 1 \\leq j \\leq m)$$\n",
    "   2. 极小型指标：$$a_{i j}^*=\\frac{\\max _{1 \\leq i \\leq n} a_{i j}-a_{i j}}{\\max _{1 \\leq i \\leq n} a_{i j}-\\min _{1 \\leq i \\leq n} a_{i j}}(1 \\leq i \\leq n, 1 \\leq j \\leq m)$$\n",
    "   3. 结果：各样本的指标值区间在0.00-1.00之间\n",
    "3. 利用熵计算各指标比重。假设$x_{ij}$指第i个样本在第j项指标下所占比重\n",
    "   1. probability p：$p_{i j}=\\frac{x_{i j}}{\\sum_{i=1}^n x_{i j}}, \\quad i=1, \\cdots, n, j=1, \\cdots, m$\n",
    "   2. information entropy: $e_j=-k \\sum_{i=1}^n p_{i j} \\ln \\left(p_{i j}\\right), \\quad j=1, \\cdots, m$ ,$k=1 / \\ln (n)>0$。熵越小，离散程度越小\n",
    "   3. 获取离散度并normalize: $$d_j=1-e_j, \\quad j=1, \\cdots, m$$\n",
    "$$w_j=\\frac{d_j}{\\sum_{j=1}^m d_j}, \\quad j=1, \\cdots, m$$\n",
    "4. 获得得分\n",
    "5. $s_i=\\sum_{j=1}^m w_j x_{i j}, \\quad i=1, \\cdots, n$\n",
    "\n",
    "注意事项：\n",
    "1. 为什么这里要除以 $\\ln (\\mathrm{n})$ 这个常数?\n",
    "在前面说过 $\\mathrm{p}\\left(\\mathrm{x}_1\\right)=\\mathrm{p}\\left(\\mathrm{x}_2\\right)=\\ldots=\\mathrm{p}\\left(\\mathrm{x}_{\\mathrm{n}}\\right)=1 / \\mathrm{n}$ 时, $\\mathrm{H}(\\mathrm{x})$ 取最大值为 $\\ln (\\mathrm{n})$, 这里除以 $\\ln (\\mathrm{n})$ 能够使得信息嫡的始终位于 $[0,1]$ 区间上面。\n",
    "1. ej 越大，即第 $\\mathrm{j}$ 个指标的信息嫡越大，表明第 $\\mathrm{j}$ 个指标的信息越多还是越少?\n",
    "答案是越少。指标相同意味着这个指标的数据没有变 化, 也就是 信息少! 因此需要将其倒转, 即计算信息效用值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100. ,  90. , 100. ,  84. ,  90. , 100. , 100. , 100. , 100. ],\n",
       "       [100. , 100. ,  78.6, 100. ,  90. , 100. , 100. , 100. , 100. ],\n",
       "       [ 75. , 100. ,  85.7, 100. ,  90. , 100. , 100. , 100. , 100. ],\n",
       "       [100. , 100. ,  78.6, 100. ,  90. , 100. ,  94.4, 100. , 100. ],\n",
       "       [100. ,  90. , 100. , 100. , 100. ,  90. , 100. , 100. ,  80. ],\n",
       "       [100. , 100. , 100. , 100. ,  90. , 100. , 100. ,  85.7, 100. ],\n",
       "       [100. , 100. ,  78.6, 100. ,  90. , 100. ,  55.6, 100. , 100. ],\n",
       "       [ 87.5, 100. ,  85.7, 100. , 100. , 100. , 100. , 100. , 100. ],\n",
       "       [100. , 100. ,  92.9, 100. ,  80. , 100. , 100. , 100. , 100. ],\n",
       "       [100. ,  90. , 100. , 100. , 100. , 100. , 100. , 100. , 100. ],\n",
       "       [100. , 100. ,  92.9, 100. ,  90. , 100. , 100. , 100. , 100. ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load data\n",
    "A = [100, 90, 100, 84, 90, 100, 100, 100, 100]\n",
    "B = [100, 100, 78.6, 100, 90, 100, 100, 100, 100]\n",
    "C = [75, 100, 85.7, 100, 90, 100, 100, 100, 100]\n",
    "D = [100, 100, 78.6, 100, 90, 100, 94.4, 100, 100]\n",
    "E = [100, 90, 100, 100, 100, 90, 100, 100, 80]\n",
    "F = [100, 100, 100, 100, 90, 100, 100, 85.7, 100]\n",
    "G = [100, 100, 78.6, 100, 90, 100, 55.6, 100, 100]\n",
    "H = [87.5, 100, 85.7, 100, 100, 100, 100, 100, 100]\n",
    "I = [100, 100, 92.9, 100, 80, 100, 100, 100, 100]\n",
    "J = [100, 90, 100, 100, 100, 100, 100, 100, 100]\n",
    "K = [100, 100, 92.9, 100, 90, 100, 100, 100, 100]\n",
    "data = [A, B, C, D, E, F, G, H, I, J, K]\n",
    "data = np.array(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weights are: [0.07578559 0.2191587  0.27137381 0.06559212 0.1051977  0.06559212\n",
      " 0.06611572 0.06559212 0.06559212]\n",
      "[[100.   90.  100.   84.   90.  100.  100.  100.  100. ]\n",
      " [100.  100.   78.6 100.   90.  100.  100.  100.  100. ]\n",
      " [ 75.  100.   85.7 100.   90.  100.  100.  100.  100. ]\n",
      " [100.  100.   78.6 100.   90.  100.   94.4 100.  100. ]\n",
      " [100.   90.  100.  100.  100.   90.  100.  100.   80. ]\n",
      " [100.  100.  100.  100.   90.  100.  100.   85.7 100. ]\n",
      " [100.  100.   78.6 100.   90.  100.   55.6 100.  100. ]\n",
      " [ 87.5 100.   85.7 100.  100.  100.  100.  100.  100. ]\n",
      " [100.  100.   92.9 100.   80.  100.  100.  100.  100. ]\n",
      " [100.   90.  100.  100.  100.  100.  100.  100.  100. ]\n",
      " [100.  100.   92.9 100.   90.  100.  100.  100.  100. ]]\n",
      "The score of each model is: [95.7069621  93.14062354 93.17273781 92.77037549 95.84064938 98.01005572\n",
      " 90.20508545 95.17203466 95.96929203 97.80841298 97.021269  ]\n"
     ]
    }
   ],
   "source": [
    "### Calculate weights from 0 to 1\n",
    "### integrate the method into a function\n",
    "def calc_weights(data_):\n",
    "    data = data_.copy()\n",
    "    ### first normalize the data\n",
    "    for i in range(data.shape[1]):\n",
    "        data[:, i] = (data[:, i] - min(data[:, i])) / (max(data[:, i]) - min(data[:, i]))\n",
    "    norm_data = data\n",
    "    ### calculate possibilities\n",
    "    possibilities = np.zeros((data.shape[0], data.shape[1]))\n",
    "    prob_log_prob = np.zeros((data.shape[0], data.shape[1]))\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            possibilities[i, j] = norm_data[i, j] / np.sum(norm_data[:, j])\n",
    "            prob_log_prob[i, j] = possibilities[i, j] * np.log(possibilities[i, j]) if possibilities[i, j] != 0 else 0\n",
    "    entropy = np.zeros(data.shape[1])\n",
    "    ### To handle the zero case, we use the prob_log_prob to calculate the entropy\n",
    "    for i in range(data.shape[1]):\n",
    "        entropy[i] = -np.sum(prob_log_prob[:, i])/np.log(data.shape[0])\n",
    "    duplicates = 1 - entropy\n",
    "    weights = duplicates / np.sum(duplicates)\n",
    "    return weights\n",
    "weights = calc_weights(data)\n",
    "print(\"The weights are: {}\".format(weights))\n",
    "### Calculate the score of each model\n",
    "print(data)\n",
    "score = np.sum(data * weights, axis=1)\n",
    "print(\"The score of each model is: {}\".format(score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 层次分析法：主观赋权"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SILENT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "33a00747c7c616a6e1f951908c7a8e16aa8bdee44e4bbac794a20c5a719e4c26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
